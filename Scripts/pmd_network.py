'''
    Takes the filtered articles generated by pmd_filter and creates a directed 
    graph out of them.
'''
import os
import pickle
import bz2
import copy
import networkx as nx

all_articles={}
os.chdir('..')
os.chdir('pmd_baseline')
for file in os.scandir():
    if 'Index' in file.name:
        pubmedentries=bz2.BZ2File(file, 'r')
        index=pickle.load(pubmedentries)
        for article in index:
            node_name=article['PMID']
            all_articles[node_name]=article
#   loading in the selected articles and decompressing
#   Creating keys for each pmid - use to construct graph

Net=nx.DiGraph()
for key in all_articles:
    Net.add_node(key)
for key in all_articles:
    article=all_articles[key]
    for citation in article['citations']:
        if str(citation) in all_articles.keys() and str(citation)!=article['PMID']:
            #this had been also returning the article itself, for some reason. never investigated why
            Net.add_edge(key,citation)
    #generating graph...

def filter_singletons(graph):
    """
    

    Parameters
    ----------
    graph : the graph of all entries by their PMIDs

    Returns
    -------
    y : how many nodes have >0 connections
    n : how many single (nonconnected) nodes there are

    """
    
    y=0
    n=0
    testnet=copy.deepcopy(graph)
    testnet=nx.Graph(testnet)
    for each_node in testnet:
        current_neighbors=Net.neighbors(each_node)
        all_neighbors=[]
        for neighbr in current_neighbors:
            all_neighbors.append(neighbr)
        if all_neighbors:
            y+=1
        if not all_neighbors:
            n+=1
            graph.remove_node(each_node)
    print('In a network:',y)
    print('Singleton:',n)
    return (y,n)
    
# getting somewhere!
#i think, dont bother trying to visualize with python standalone. use networkx for
# its graph objects and support and then export the result to cytoscape for vis...

#nothing much left to do now but load up everything...
# for final filter, try looking at neighbors of neighbors,...& etc to root out small
#disconnected networks. then, assess nodes by accessibility metrics (SAWs?). also research
#centrality measurements
firstfilter=filter_singletons(graph=Net)
# longest=[{'0'}]
total_subgraphs=0
for item in nx.connected_components(nx.Graph(Net)):
    total_subgraphs+=1
    # for subgraph in longest:
    #     if len(item)>len(subgraph):
    #         longest.append(item)
    #         longest.remove(longest[0])
    if len(item)<10 and len(item)>0:
    #     # for node in item:
    #     #     print(all_articles[node])
    #     # prompt=input('Reject network? [y/n]')
    #     # if prompt == 'y':
        for node in item:
            edge_removal=[]
            for neighbor in Net.neighbors(node):
                edge_removal.append(neighbor)
            for edge in edge_removal:
                Net.remove_edge(node,edge)
            try:
                Net.remove_node(node)
            except:
                continue
        else:
            continue
        #removing small subgraphs. thresholds here can be adjusted.
postfilter_subgraphs=0
postfilter_nodes=0
for item in nx.connected_components(nx.Graph(Net)):
    postfilter_subgraphs+=1
    postfilter_nodes+=len(item)
    print(item)
print ('Number of subgraphs passing filter:',postfilter_subgraphs,'/',total_subgraphs)
print('Nodes represented:',postfilter_nodes,'/',firstfilter[0])
# as it turns out (from actually reading the documentation) you can just use max(nx.conncomp)
#to find the largest subgraph. since i've already written it this way and I also included
#the filter of everything else i'm leaving it as is
def make_subgraphs(Net):
    for item in nx.connected_components(nx.Graph(Net)):
        print(item)
        subg=nx.subgraph(Net,item)
        yield subg
#nx.draw_networkx(Net)
# ^this is useless, creates a hairball

#so, this can let me sort for the largest subgraphs - or rather, review and filter out
# small irrelevant ones. problem may arise if there are multiple large(>a few thousand)
#graphs that are relevant. or, how would you determine + discriminate their relevance?
# ...looks like it is finding all subgraphs fine, it just wont graph anything smaller than 
#like, 10 nodes or something like that

#ok now to assess importantness of each node within the graph.
def most_central(Net):
    #there are many ways to do this and they are all computationally expensive
    x=nx.betweenness_centrality(Net)
    central=max(x, key=lambda key:x[key])
        #this returns the key of the highest centrality valued node
    #what if I wanted an ordered list?
    i=0
    for thingy in all_articles:
        #i know, i know, my naming conventions are horrible
        morestuff=all_articles[thingy]
        if central in morestuff['citations']:
            i+=1
    print('the most central node is',central+ ', which is cited',i,'times')
#centrality=most_central(Net)
#this takes a min or two to compute...or ten

download_graph=bz2.BZ2File('Net','w')
pickle.dump(Net,download_graph)
download_graph.close()
download_all_articles=bz2.BZ2File('index','w')
pickle.dump(all_articles, download_all_articles)
download_all_articles.close()
#download_centrality=bz2.BZ2File('Centrality','w')
#pickle.dump(centrality, download_centrality)
#download_centrality.close()
pubmedentries.close()
